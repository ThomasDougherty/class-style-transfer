{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "stylish-people",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM71VCvuEyBDLb+9Lq0CaHW",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "338b55c5e0564c82be396b0324686bec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9250578b806443ab85ea2c46361f23ba",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_8730781e3700455fa1ed3eb199685bbe",
              "IPY_MODEL_ddee6bf0f6ca4e398bcc39dd99c3025e"
            ]
          }
        },
        "9250578b806443ab85ea2c46361f23ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8730781e3700455fa1ed3eb199685bbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_5e38569b01f14d369880430a9ee2dedb",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 25020748,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 25020748,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6611d3661ac543c290cdac495103eb25"
          }
        },
        "ddee6bf0f6ca4e398bcc39dd99c3025e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_908a9ad2c45d488a83d42fa4ecfb648e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 23.9M/23.9M [00:00&lt;00:00, 38.2MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_27c5135470494abcafbee546664950b2"
          }
        },
        "5e38569b01f14d369880430a9ee2dedb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6611d3661ac543c290cdac495103eb25": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "908a9ad2c45d488a83d42fa4ecfb648e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "27c5135470494abcafbee546664950b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ThomasDougherty/stylish-people/blob/main/stylish_people.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWkEh1wXLiul"
      },
      "source": [
        "# Welcome!\n",
        "## This is a collaborative notebook for class specific style transfers\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQBEw61pUjPN",
        "cellView": "form"
      },
      "source": [
        "#@title Select options\n",
        "\n",
        "PEOPLE = True #@param {type:\"boolean\"}\n",
        "SKATEBOARD = False #@param {type:\"boolean\"}\n",
        "OTHER_CLASSES = '' #@param {type:\"string\"}\n",
        "STYLE = \"mosaic\" #@param [\"candy\", \"mosaic\", \"rain_princess\", \"udnie\"]\n",
        "\n",
        "INVERSE = False #@param {type:\"boolean\"}\n",
        "CLASS_SPECIFIC = True #@param {type:\"boolean\"}\n",
        "\n",
        "STYLE_PTH = \"saved_models/\" + STYLE + \".pth\"\n",
        "\n",
        "objects = []\n",
        "if PEOPLE:\n",
        "  objects.append(\"person\")\n",
        "if SKATEBOARD:\n",
        "  objects.append(\"skateboard\")\n",
        "if OTHER_CLASSES:\n",
        "  OTHER_CLASSES = OTHER_CLASSES.replace(\" \", \"\")\n",
        "  other_list = OTHER_CLASSES.split(\",\")"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "9zeJRA08Wiow",
        "cellView": "form",
        "outputId": "1e914b00-9d97-4e21-94c6-c21070796587"
      },
      "source": [
        "#@title Select file(s)\n",
        "\n",
        "from google.colab import files\n",
        "files_list = files.upload()\n",
        "\n",
        "import sys\n",
        "!pip install filetype\n",
        "import filetype\n",
        "\n",
        "media_formats = [\"video\", \"image\"]\n",
        "for key in files_list:\n",
        "  kind = filetype.guess(key)\n",
        "  file_type = kind.mime.split(\"/\")[0]\n",
        "  if file_type not in media_formats:\n",
        "    print(\"Unrecognized media file: \" + key)\n",
        "    sys.exit()\n",
        "  print(kind.mime)\n",
        "  print(key)\n",
        "  file_name = key"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-abd947e1-7834-4dc2-93d0-ea9282d74de4\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-abd947e1-7834-4dc2-93d0-ea9282d74de4\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving IMG_6296.mov to IMG_6296.mov\n",
            "Requirement already satisfied: filetype in /usr/local/lib/python3.6/dist-packages (1.0.7)\n",
            "video/quicktime\n",
            "IMG_6296.mov\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83,
          "referenced_widgets": [
            "338b55c5e0564c82be396b0324686bec",
            "9250578b806443ab85ea2c46361f23ba",
            "8730781e3700455fa1ed3eb199685bbe",
            "ddee6bf0f6ca4e398bcc39dd99c3025e",
            "5e38569b01f14d369880430a9ee2dedb",
            "6611d3661ac543c290cdac495103eb25",
            "908a9ad2c45d488a83d42fa4ecfb648e",
            "27c5135470494abcafbee546664950b2"
          ]
        },
        "id": "ByTl-QGPmhdu",
        "cellView": "form",
        "outputId": "a63edf03-124e-4bdd-b212-2f96d0c8ff62"
      },
      "source": [
        "#@title Collecting stuff...\n",
        "# import some common \n",
        "import json\n",
        "import os\n",
        "import re\n",
        "import random\n",
        "import subprocess\n",
        "\n",
        "import cv2\n",
        "import filetype\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "!pip install pyyaml==5.1\n",
        "# install detectron2: (Colab has CUDA 10.1 + torch 1.7)\n",
        "# See https://detectron2.readthedocs.io/tutorials/install.html for instructions\n",
        "import torch\n",
        "assert torch.__version__.startswith(\"1.7\")\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.7/index.html &> /dev/null\n",
        "# Some basic setup:\n",
        "# Setup detectron2 logger\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "\n",
        "# import some common detectron2 utilities\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
        "\n",
        "# Installing pretrained model weights\n",
        "import zipfile\n",
        "\n",
        "def unzip(source_filename, dest_dir):\n",
        "    with zipfile.ZipFile(source_filename) as zf:\n",
        "        zf.extractall(path=dest_dir)\n",
        "\n",
        "try:\n",
        "    from torch.utils.model_zoo import _download_url_to_file\n",
        "except ImportError:\n",
        "    try:\n",
        "        from torch.hub import download_url_to_file as _download_url_to_file\n",
        "    except ImportError:\n",
        "        from torch.hub import _download_url_to_file\n",
        "\n",
        "_download_url_to_file('https://www.dropbox.com/s/lrvwfehqdcxoza8/saved_models.zip?dl=1', 'saved_models.zip', None, True)\n",
        "unzip('saved_models.zip', '.')\n",
        "\n",
        "# Creating directories\n",
        "!mkdir -p finished_products\n",
        "\n",
        "FINISHED_PRODUCTS = \"finished_products/\"\n",
        "TEMP_DIR = \"temp_imgs/\"\n",
        "ORG_DIR = \"temp_imgs/org_imgs/\"\n",
        "STYLE_DIR = \"temp_imgs/style_imgs/\"\n",
        "MASKS_DIR = \"temp_imgs/masks_imgs/\"\n",
        "FINAL_DIR = \"temp_imgs/final_imgs/\""
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyyaml==5.1 in /usr/local/lib/python3.6/dist-packages (5.1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "338b55c5e0564c82be396b0324686bec",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=25020748.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cp-SPX1ShoQE",
        "cellView": "form"
      },
      "source": [
        "#@title Utilities\n",
        "\n",
        "def create_mask(outputs, num_class_list, img):\n",
        "  '''Creates binary masks of detected objects'''\n",
        "  class_pred = outputs[\"instances\"].pred_classes.detach().cpu().numpy()\n",
        "  masks_pred = outputs[\"instances\"].pred_masks.detach().cpu().numpy()\n",
        "  bin_mask = np.zeros(img.shape[:2])\n",
        "  if len(class_pred) != 0:\n",
        "    for cnt, c in enumerate(class_pred):\n",
        "      if c in num_class_list:\n",
        "        bin_mask += masks_pred[cnt]*1\n",
        "  bin_mask[bin_mask > 0] = 1\n",
        "  return bin_mask\n",
        "\n",
        "def extract_frames(file_name):\n",
        "  '''Extract frames from video \"file_name\" '''\n",
        "  vidcap = cv2.VideoCapture(file_name)\n",
        "  success,image = vidcap.read()\n",
        "  frame_num = 0\n",
        "  img_list = []\n",
        "  base_names = []\n",
        "  while success:\n",
        "      cv2.imwrite(ORG_DIR + \"frame_{0:05}.png\".format(frame_num), image)\n",
        "      success,image = vidcap.read() \n",
        "      base_names.append(\"frame_{0:05}.png\".format(frame_num))       \n",
        "      frame_num += 1            \n",
        "  base_names.sort()\n",
        "  return base_names, vidcap\n",
        "\n",
        "def final_imgs(base_names):\n",
        "  '''Combines the style and mask images to create final frames.'''\n",
        "  for base in tqdm(base_names):\n",
        "      if CLASS_SPECIFIC:\n",
        "        frame_img = cv2.imread(ORG_DIR + base)\n",
        "        style_img = cv2.imread(STYLE_DIR + base)\n",
        "        mask_img = cv2.imread(MASKS_DIR + base)\n",
        "\n",
        "        if style_img.shape != frame_img.shape:\n",
        "          f_w, f_h, c = frame_img.shape\n",
        "          style_img = cv2.resize(style_img, (f_h, f_w), interpolation = cv2.INTER_AREA)\n",
        "        if INVERSE:\n",
        "          frame_img[mask_img==0] = 0\n",
        "          style_img[mask_img==1] = 0\n",
        "        else:\n",
        "          frame_img[mask_img==1] = 0\n",
        "          style_img[mask_img==0] = 0\n",
        "        final_img = frame_img + style_img\n",
        "        cv2.imwrite(FINAL_DIR + base, final_img)\n",
        "      else:\n",
        "        style_img = cv2.imread(STYLE_DIR + base)\n",
        "        cv2.imwrite(FINAL_DIR + base, style_img)\n",
        "  return\n",
        "\n",
        "def create_vid(base_names, vidcap, save_name):\n",
        "  '''Creates video from final frames'''\n",
        "  img = cv2.imread(ORG_DIR + base_names[0])\n",
        "  height, width, layers = img.shape\n",
        "  size = (width,height)\n",
        "\n",
        "  fps = vidcap.get(cv2.CAP_PROP_FPS)\n",
        "  out = cv2.VideoWriter(TEMP_DIR + 'vid_nosound.mp4',  #Provide a file to write the video to\n",
        "                        cv2.VideoWriter_fourcc(*'DIVX'),\n",
        "                        round(fps),                                        \n",
        "                        size)\n",
        "  for base in tqdm(base_names):\n",
        "    img = cv2.imread(FINAL_DIR + base)\n",
        "    out.write(img)\n",
        "  out.release()\n",
        "\n",
        "  command = \"ffmpeg -i temp_imgs/final_imgs/frame_%05d.png -c:v libx264 -vf fps=\" + str(round(fps)) + \" \" + TEMP_DIR + 'vid_nosound.mp4'\n",
        "  subprocess.call(command, shell=True)\n",
        "\n",
        "  command = \"ffmpeg -i \" + file_name + \" -ab 160k -ac 2 -ar 44100 -vn \" + TEMP_DIR + \"audio.wav\"\n",
        "  subprocess.call(command, shell=True)\n",
        "\n",
        "  command = \"ffmpeg -i \" + TEMP_DIR + 'vid_nosound.mp4' + \" -i \" + TEMP_DIR + \"audio.wav -c:v copy -c:a aac \" + TEMP_DIR + \"vid_b_conv.mp4\"\n",
        "  subprocess.call(command, shell=True)\n",
        "\n",
        "  if save_name is None:\n",
        "    save_name = file_name + \"style\"\n",
        "  command = \"ffmpeg -i \" + TEMP_DIR + \"vid_b_conv.mp4 -vcodec libx264 -profile:v main -level 3.1 -preset medium -crf 23 -x264-params ref=4 -acodec copy -movflags +faststart  \" + FINISHED_PRODUCTS + save_name + \".mp4\"\n",
        "  subprocess.call(command, shell=True)\n",
        "  return\n",
        "\n",
        "def remove_temp():\n",
        "  ! rm -r temp_imgs\n",
        "  return\n",
        "\n",
        "def create_temp():\n",
        "  !mkdir -p temp_imgs\n",
        "  !mkdir -p temp_imgs/org_imgs/\n",
        "  !mkdir -p temp_imgs/style_imgs/\n",
        "  !mkdir -p temp_imgs/masks_imgs/\n",
        "  !mkdir -p temp_imgs/final_imgs/\n",
        "  return\n",
        "\n",
        "def create_name(key):\n",
        "  base = key.split(\".\")[0] + \"_\" + STYLE\n",
        "  if INVERSE:\n",
        "    base = base + \"_INVERSE\"\n",
        "  if not CLASS_SPECIFIC:\n",
        "    base = base + \"_NOTSPECIFIC\"\n",
        "  for ob in objects:\n",
        "    base = base + \"_\" + ob\n",
        "  return base + \"_\" + timestamp\n"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5RCeG4UVKUm",
        "cellView": "form"
      },
      "source": [
        "#@title Style transfer models\n",
        "class TransformerNet(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TransformerNet, self).__init__()\n",
        "        # Initial convolution layers\n",
        "        self.conv1 = ConvLayer(3, 32, kernel_size=9, stride=1)\n",
        "        self.in1 = torch.nn.InstanceNorm2d(32, affine=True)\n",
        "        self.conv2 = ConvLayer(32, 64, kernel_size=3, stride=2)\n",
        "        self.in2 = torch.nn.InstanceNorm2d(64, affine=True)\n",
        "        self.conv3 = ConvLayer(64, 128, kernel_size=3, stride=2)\n",
        "        self.in3 = torch.nn.InstanceNorm2d(128, affine=True)\n",
        "        # Residual layers\n",
        "        self.res1 = ResidualBlock(128)\n",
        "        self.res2 = ResidualBlock(128)\n",
        "        self.res3 = ResidualBlock(128)\n",
        "        self.res4 = ResidualBlock(128)\n",
        "        self.res5 = ResidualBlock(128)\n",
        "        # Upsampling Layers\n",
        "        self.deconv1 = UpsampleConvLayer(128, 64, kernel_size=3, stride=1, upsample=2)\n",
        "        self.in4 = torch.nn.InstanceNorm2d(64, affine=True)\n",
        "        self.deconv2 = UpsampleConvLayer(64, 32, kernel_size=3, stride=1, upsample=2)\n",
        "        self.in5 = torch.nn.InstanceNorm2d(32, affine=True)\n",
        "        self.deconv3 = ConvLayer(32, 3, kernel_size=9, stride=1)\n",
        "        # Non-linearities\n",
        "        self.relu = torch.nn.ReLU()\n",
        "\n",
        "    def forward(self, X):\n",
        "        y = self.relu(self.in1(self.conv1(X)))\n",
        "        y = self.relu(self.in2(self.conv2(y)))\n",
        "        y = self.relu(self.in3(self.conv3(y)))\n",
        "        y = self.res1(y)\n",
        "        y = self.res2(y)\n",
        "        y = self.res3(y)\n",
        "        y = self.res4(y)\n",
        "        y = self.res5(y)\n",
        "        y = self.relu(self.in4(self.deconv1(y)))\n",
        "        y = self.relu(self.in5(self.deconv2(y)))\n",
        "        y = self.deconv3(y)\n",
        "        return y\n",
        "\n",
        "\n",
        "class ConvLayer(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride):\n",
        "        super(ConvLayer, self).__init__()\n",
        "        reflection_padding = kernel_size // 2\n",
        "        self.reflection_pad = torch.nn.ReflectionPad2d(reflection_padding)\n",
        "        self.conv2d = torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.reflection_pad(x)\n",
        "        out = self.conv2d(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResidualBlock(torch.nn.Module):\n",
        "    \"\"\"ResidualBlock\n",
        "    introduced in: https://arxiv.org/abs/1512.03385\n",
        "    recommended architecture: http://torch.ch/blog/2016/02/04/resnets.html\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, channels):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = ConvLayer(channels, channels, kernel_size=3, stride=1)\n",
        "        self.in1 = torch.nn.InstanceNorm2d(channels, affine=True)\n",
        "        self.conv2 = ConvLayer(channels, channels, kernel_size=3, stride=1)\n",
        "        self.in2 = torch.nn.InstanceNorm2d(channels, affine=True)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.relu(self.in1(self.conv1(x)))\n",
        "        out = self.in2(self.conv2(out))\n",
        "        out = out + residual\n",
        "        return out\n",
        "\n",
        "\n",
        "class UpsampleConvLayer(torch.nn.Module):\n",
        "    \"\"\"UpsampleConvLayer\n",
        "    Upsamples the input and then does a convolution. This method gives better results\n",
        "    compared to ConvTranspose2d.\n",
        "    ref: http://distill.pub/2016/deconv-checkerboard/\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride, upsample=None):\n",
        "        super(UpsampleConvLayer, self).__init__()\n",
        "        self.upsample = upsample\n",
        "        reflection_padding = kernel_size // 2\n",
        "        self.reflection_pad = torch.nn.ReflectionPad2d(reflection_padding)\n",
        "        self.conv2d = torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_in = x\n",
        "        if self.upsample:\n",
        "            x_in = torch.nn.functional.interpolate(x_in, mode='nearest', scale_factor=self.upsample)\n",
        "        out = self.reflection_pad(x_in)\n",
        "        out = self.conv2d(out)\n",
        "        return out"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvFQzOjJmspr",
        "cellView": "form"
      },
      "source": [
        "#@title Engine\n",
        "\n",
        "def run_engine(base_names):\n",
        "  '''Runs the stylization and detection models on frames'''\n",
        "  # stylization setup\n",
        "  content_transform = transforms.Compose([\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Lambda(lambda x: x.mul(255))\n",
        "  ])\n",
        "  with torch.no_grad():\n",
        "      style_model = TransformerNet()\n",
        "      state_dict = torch.load(STYLE_PTH)\n",
        "      # remove saved deprecated running_* keys in InstanceNorm from the checkpoint\n",
        "      for k in list(state_dict.keys()):\n",
        "          if re.search(r'in\\d+\\.running_(mean|var)$', k):\n",
        "              del state_dict[k]\n",
        "      style_model.load_state_dict(state_dict)\n",
        "      style_model.to(device='cuda:0')\n",
        "  style_imgs = []\n",
        "\n",
        "  if CLASS_SPECIFIC:\n",
        "    # Detectron2 setup\n",
        "    cfg = get_cfg()\n",
        "    # add project-specific config (e.g., TensorMask) here if you're not running a model in detectron2's core library\n",
        "    cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_X_101_32x8d_FPN_3x.yaml\"))\n",
        "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model\n",
        "    # Find a model from detectron2's model zoo. You can use the https://dl.fbaipublicfiles... url as well\n",
        "    cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_X_101_32x8d_FPN_3x.yaml\")\n",
        "    predictor = DefaultPredictor(cfg)\n",
        "\n",
        "    # get class list \n",
        "    img = cv2.imread(ORG_DIR + base_names[0])\n",
        "    v = Visualizer(img[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
        "    class_list = v.metadata.thing_classes\n",
        "\n",
        "    # convert object classes to index\n",
        "    num_class_list = []\n",
        "    for ob in objects:\n",
        "      num_class_list.append(class_list.index(ob))\n",
        "\n",
        "  for base in tqdm(base_names):\n",
        "    img = cv2.imread(ORG_DIR + base)\n",
        "\n",
        "    if CLASS_SPECIFIC:\n",
        "      outputs = predictor(img)\n",
        "      binary_mask = create_mask(outputs, num_class_list, img)\n",
        "      cv2.imwrite(MASKS_DIR + base, binary_mask)\n",
        "\n",
        "    content_image = img\n",
        "    content_image = content_transform(content_image)\n",
        "    content_image = content_image.unsqueeze(0).to(device='cuda:0')\n",
        "    with torch.no_grad():\n",
        "      style_img = style_model(content_image)\n",
        "      style_img = torch.squeeze(style_img)\n",
        "      style_img = style_img.permute(1, 2, 0).detach().cpu().numpy()\n",
        "    cv2.imwrite(STYLE_DIR + base, style_img) \n",
        "  return"
      ],
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rW47yDC1ggkR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "475f2472-656c-45e8-c186-69eaf60b49a5"
      },
      "source": [
        "#@title Main\n",
        "import datetime\n",
        "! pip install pyheif\n",
        "import pyheif\n",
        "import PIL\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "is_video = None\n",
        "for key in files_list:\n",
        "  print(\"Stylizing \" + key)\n",
        "  create_temp()\n",
        "  kind = filetype.guess(key)\n",
        "  file_type = kind.mime.split(\"/\")[0]\n",
        "  if file_type == \"video\":\n",
        "    is_video = True\n",
        "  else:\n",
        "    is_video = False\n",
        "  file_name = key\n",
        "\n",
        "  timestamp = datetime.datetime.now().strftime('%Y%m%d%H%M%S')\n",
        "\n",
        "  save_name = create_name(key)\n",
        "\n",
        "  # if video extract frames, else move image into ORG_DIR\n",
        "  \n",
        "  if is_video:\n",
        "    print(\"Extracting Frames...\")\n",
        "    base_names, vidcap = extract_frames(file_name)\n",
        "  else:\n",
        "    if file_name.split(\".\")[1].lower() == 'heic':\n",
        "      heif_file = pyheif.read(file_name)\n",
        "      image = Image.frombytes(\n",
        "          heif_file.mode, \n",
        "          heif_file.size, \n",
        "          heif_file.data,\n",
        "          \"raw\",\n",
        "          heif_file.mode,\n",
        "          heif_file.stride,\n",
        "          )\n",
        "      open_cv_image = np.array(image) \n",
        "      # Convert RGB to BGR \n",
        "      open_cv_image = open_cv_image[:, :, ::-1].copy() \n",
        "      cv2.imwrite(ORG_DIR + file_name.split(\".\")[0] + \".png\", open_cv_image)\n",
        "      base_names = [file_name.split(\".\")[0] + \".png\"]\n",
        "    else:\n",
        "      img = cv2.imread(file_name)\n",
        "      cv2.imwrite(ORG_DIR + file_name, img)\n",
        "      base_names = [file_name]\n",
        "\n",
        "  print(\"Running Engine...\")\n",
        "  run_engine(base_names)\n",
        "  print(\"Finalizing images...\")\n",
        "  final_imgs(base_names)\n",
        "  if is_video:\n",
        "    print(\"Generating video...\")\n",
        "    create_vid(base_names, vidcap, save_name)\n",
        "  else:\n",
        "    img = cv2.imread(FINAL_DIR + base_names[0])\n",
        "    cv2.imwrite(FINISHED_PRODUCTS + save_name + \".png\", img)\n",
        "  remove_temp()\n",
        "  command = \"rm \" + file_name\n",
        "  subprocess.call(command, shell=True)"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyheif in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "Requirement already satisfied: cffi>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from pyheif) (1.14.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.0.0->pyheif) (2.20)\n",
            "Stylizing IMG_6296.mov\n",
            "Extracting Frames...\n",
            "Running Engine...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 238/238 [03:32<00:00,  1.12it/s]\n",
            "  0%|          | 0/238 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Finalizing images...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 238/238 [00:52<00:00,  4.54it/s]\n",
            "  1%|          | 2/238 [00:00<00:15, 15.10it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generating video...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 238/238 [00:16<00:00, 14.70it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}